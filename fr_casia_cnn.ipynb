{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7710309,
          "sourceType": "datasetVersion",
          "datasetId": 4502072
        }
      ],
      "dockerImageVersionId": 30702,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "fr_casia_cnn",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Br-alexis73/Face_recognition/blob/master/fr_casia_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'casia-face-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4502072%2F7710309%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240420%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240420T222148Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D63e9d4210e230cac835476e827d8feb252647573aa6956d25737a1a3da52b7a473e3fdbe7b82c96e68971db2ddb6ca22ae18c018ceaa74d4febc534e9c7316e1028a53f2d1b54dbe082940661ae0138f20dbb1ba05603b5ab7eb4c5b5c946e834a42d226bc80372359c5c39db97c76e0a8c2a953cd452d5cf2456505a3e0470c22f8c2c7e0a4b69c72ce722a26232ee46eaad8009f62ccf1fcffb3a38a045e8b3d94a91c23770c2354150d5265447792467bac3814c51de2a5cd35bcc97fec90523ef71d0eca6c5ab496335290f5d1cd8d300cc3c998f180720ce18bab96bf8b98ade4f089df20b2cb707156c8395502a409eaf5ce2d71c1329a078f5c7837e0'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "4MmTkmZMmfpq"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Set the dataset path to your Kaggle dataset location\n",
        "dataset_path = '../input/casia-face-dataset/casia dataset'"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-20T22:15:07.392087Z",
          "iopub.execute_input": "2024-04-20T22:15:07.393148Z",
          "iopub.status.idle": "2024-04-20T22:15:24.537211Z",
          "shell.execute_reply.started": "2024-04-20T22:15:07.39311Z",
          "shell.execute_reply": "2024-04-20T22:15:24.535823Z"
        },
        "trusted": true,
        "id": "SqngyWVlmfps",
        "outputId": "5a9ea033-45e6-41da-cb08-3cd97031426d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-04-20 22:15:12.600380: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-20 22:15:12.600514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-20 22:15:12.779846: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a list of (id, image count) tuples\n",
        "ids_images = []\n",
        "\n",
        "# Loop over each person's directory\n",
        "for person_id in os.listdir(dataset_path):\n",
        "    directory = os.path.join(dataset_path, person_id)\n",
        "    if os.path.isdir(directory):\n",
        "        images = [i for i in os.listdir(directory) if i.endswith('.jpg')]\n",
        "        image_count = len(images)\n",
        "        # Add to list if there are a minimum number of images (e.g., 50)\n",
        "        if image_count >= 50:\n",
        "            ids_images.append((person_id, min(image_count, 50)))\n",
        "\n",
        "# Create a DataFrame with ids and image counts\n",
        "people_df = pd.DataFrame(ids_images, columns=['id', 'images'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T22:15:34.443467Z",
          "iopub.execute_input": "2024-04-20T22:15:34.444348Z",
          "iopub.status.idle": "2024-04-20T22:15:36.72182Z",
          "shell.execute_reply.started": "2024-04-20T22:15:34.444302Z",
          "shell.execute_reply": "2024-04-20T22:15:36.720867Z"
        },
        "trusted": true,
        "id": "5JrRrZUCmfpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image size for CNN input\n",
        "im_size = 224\n",
        "\n",
        "# Initialize lists to store the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Load and preprocess images\n",
        "for person_id, _ in ids_images:\n",
        "    person_directory = os.path.join(dataset_path, person_id)\n",
        "    person_images = sorted([os.path.join(person_directory, i) for i in os.listdir(person_directory) if i.endswith('.jpg')])[:50]\n",
        "    for image_path in person_images:\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.resize(img, (im_size, im_size))\n",
        "        img = img.astype('float32') / 255.0\n",
        "        images.append(img)\n",
        "        labels.append(person_id)\n",
        "#\n",
        "# Convert to numpy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Shuffle and split dataset\n",
        "images, labels = shuffle(images, labels, random_state=1)\n",
        "train_x, test_x, train_y, test_y = train_test_split(images, y_onehot, test_size=0.3, random_state=415)\n",
        "\n",
        "# Output shapes to verify\n",
        "print(\"train_x shape:\", train_x.shape)\n",
        "print(\"train_y shape:\", train_y.shape)\n",
        "print(\"test_x shape:\", test_x.shape)\n",
        "print(\"test_y shape:\", test_y.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T22:19:48.592592Z",
          "iopub.execute_input": "2024-04-20T22:19:48.593073Z",
          "iopub.status.idle": "2024-04-20T22:20:35.201007Z",
          "shell.execute_reply.started": "2024-04-20T22:19:48.59304Z",
          "shell.execute_reply": "2024-04-20T22:20:35.199213Z"
        },
        "trusted": true,
        "id": "jgvGEhohmfpt",
        "outputId": "655be719-c8b4-4ab2-b390-571e678a7368"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Shuffle and split dataset\u001b[39;00m\n\u001b[1;32m     24\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m shuffle(images, labels, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m train_x, test_x, train_y, test_y \u001b[38;5;241m=\u001b[39m train_test_split(images, \u001b[43my_onehot\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m415\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Output shapes to verify\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_x shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_x\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_onehot' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'y_onehot' is not defined",
          "output_type": "error"
        }
      ]
    }
  ]
}